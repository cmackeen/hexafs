{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hexafs_preprocess_v2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNCb6RMcKKcy",
        "colab_type": "text"
      },
      "source": [
        "# Unsupervised grouping application of XAS spectra using pyspark, scikit-learn, and dash\n",
        "\n",
        "### Motivation\n",
        "My advisor at UC Santa Cruz has decades worth of *manually processed* XAS data, specifically EXAFS data. As I cranked through material research learning tips from my advisor, I dreamt of the potential of his decades of work reducing data as building a training set for a highly technical analysis. Before I got too far ahead of myself, I wanted to devise a realistic goal that will be part of the foundation for my loftier ideas. \n",
        "\n",
        "Enter unsupervised clustering. I thought it would be useful for anyone, from XAS experts to undergraduates, to have an automatic classifier that could detect a \"new\" spectrum that differs from previously collected data. This detection could relate to a new material, an experimental error, novel behavior, a phase transition, etc. \n",
        "\n",
        "To do this with a subset of the Bridges Historical EXAFS Database I wanted to approach with scalability in mind for future use. This is why I opted to delve into using Apache Spark, specifically pyspark. While the use of pyspark for this small dataframe (and loading the normalized mfcc dataframe) is not necessary, it serves as a starting point for scaling, as well as educational for people working on similar frameworks. \n",
        "\n",
        "To get pyspark running in a Colab environment\n",
        "I am following https://towardsdatascience.com/pyspark-in-google-colab-6821c2faf41c except for the fact I am wget'ing an archvie spark, see link difference . . .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCB1VbiOxAkE",
        "colab_type": "code",
        "outputId": "38ead5ff-1413-48d8-95f6-487d49523d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-26 22:47:48--  http://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 163.172.17.199\n",
            "Connecting to archive.apache.org (archive.apache.org)|163.172.17.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226027370 (216M) [application/x-gzip]\n",
            "Saving to: ‘spark-2.3.3-bin-hadoop2.7.tgz.2’\n",
            "\n",
            "spark-2.3.3-bin-had 100%[===================>] 215.56M  9.18MB/s    in 27s     \n",
            "\n",
            "2020-02-26 22:48:16 (8.07 MB/s) - ‘spark-2.3.3-bin-hadoop2.7.tgz.2’ saved [226027370/226027370]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdCgehyyJlH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get java installed\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xvf spark-2.3.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql0tXeBFKHwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.3-bin-hadoop2.7\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KveALouwK2ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaHMHDXByRZr",
        "colab_type": "text"
      },
      "source": [
        "Then we load the mini hEXAFS database from my github:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwIP3vu9K48M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget \"http://exafs.ucsc.edu/bridges/data_agg_mini.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpH5OrgqLmeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "dataset = spark.read.load(\"data_agg_mini.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\",parserLib=\"univocity\",multiLine=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwNdDP4oyfDy",
        "colab_type": "text"
      },
      "source": [
        "general dataframe overview, make sure multiline worked for e,mu arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEJRl3BkNS_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQaH6Z6qyrNP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "now strip all the \"\\n's\" from the array\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2BvritTzAzZ",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "ele_selector=\"Fe\"\n",
        "df=dataset.na.drop(subset=[\"5\"])\n",
        "\n",
        "df=df.withColumn('5', regexp_replace('5', '[\\n\\[\\]]', ' '))\n",
        "df=df.withColumn('6', regexp_replace('6', '[\\n\\[\\]]', ' '))\n",
        "df2=df.withColumn(\"5\", split(\"5\", \"\\s\"))\n",
        "df2=df2.withColumn(\"6\", split(\"6\", \"\\s\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q2GkBT4OzANP"
      },
      "source": [
        "## cleaning begins\n",
        "\n",
        "We start with cutting to region of interest for each spectra, and casting the string array as an array of floats, which we can operate on\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aQRVuM5my_bU",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.column import _to_java_column, _to_seq, Column\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType, FloatType,IntegerType\n",
        "from pyspark.sql.functions import array\n",
        "import numpy as np\n",
        "\n",
        "#cast string array as an array of floats\n",
        "df2=df2.withColumn(\"5\", df2[\"5\"].cast('array<float>'))\n",
        "df2=df2.withColumn(\"6\", df2[\"6\"].cast('array<float>'))\n",
        "\n",
        "#useful for checking the lengths match between e,mu after processing\n",
        "slen = udf(lambda s: len(s), IntegerType())\n",
        "\n",
        "# Removes 'none' entries in lists of e, mu\n",
        "nonone = udf(lambda s: [s for s in s if s is not None], 'array<float>')\n",
        "\n",
        "# Cuts out the pre-edge region\n",
        "def edgecutter(t,r):\n",
        "  try:\n",
        "    diff = [j-i for i, j in zip(t[:-1], t[1:])]\n",
        "    cl=diff.index(np.max(diff))\n",
        "    if cl + 50 < len(diff):\n",
        "      cl=diff.index(np.max(diff))+50\n",
        "    else:\n",
        "      cl=diff.index(np.max(diff))\n",
        "\n",
        "  except:\n",
        "    cl=0\n",
        "  return r[cl:]\n",
        "\n",
        "# apply functinos to arrays in e,mu columns\n",
        "adf = df2.withColumn(\"5\", nonone(df2[\"5\"]))\n",
        "adf = adf.withColumn(\"6\", nonone(adf[\"6\"]))\n",
        "adf = adf.withColumn(\"ecnt\", slen(adf[\"5\"]))\n",
        "adf = adf.withColumn(\"mucnt\", slen(adf[\"6\"]))\n",
        "\n",
        "# Check mu and e are still the same length\n",
        "bdf = adf.filter(adf[\"mucnt\"]==adf[\"ecnt\"])\n",
        "\n",
        "# Init and apply pyspark udf for cutting of pre-edge\n",
        "edgecut_udf=udf(edgecutter, 'array<float>')\n",
        "cdf=bdf.withColumn(\"ecut\", edgecut_udf(bdf[\"6\"],bdf[\"5\"]))\n",
        "cdf=cdf.withColumn(\"mucut\", edgecut_udf(cdf[\"6\"],cdf[\"6\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttvrHZnMqBgu",
        "colab_type": "text"
      },
      "source": [
        "Now we zero the energy and subtract the mean of mu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m6XpSh6Zy6fV",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# This is the df->RDD->df wash cycle\n",
        "# Washes off the scalar udf stench ...\n",
        "# this issue needs to be adressed as PySpark development improves VectorUDF functionality\n",
        "\n",
        "rdd = cdf.rdd.map(list)\n",
        "zdf=rdd.toDF()\n",
        "\n",
        "@pandas_udf('array<float>')\n",
        "def min_off(v):\n",
        "    return pd.Series(v).apply(lambda x: x-np.min(x))\n",
        "\n",
        "@pandas_udf('array<float>')\n",
        "def mean_off(v):\n",
        "    return pd.Series(v).apply(lambda x: x-np.mean(x))\n",
        "\n",
        "zdf1=zdf.withColumn(\"e_zset\", min_off(zdf[\"_14\"]))\n",
        "zdf2=zdf1.withColumn(\"mu_zset\", mean_off(zdf1[\"_15\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL7cgF4ktEfh",
        "colab_type": "text"
      },
      "source": [
        "Fit background spline, and subtract it from resampled spectra. \n",
        "We save the normalized data to be used in the Dash app plot of categorized, normalized EXAFS scans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW4cLJdKwGV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## Trying to work spline fitting into a pysparkable udf, or maybe pandas udf. 2 arrays in, returns just m\n",
        "from scipy.interpolate import splrep, splev, LSQUnivariateSpline, UnivariateSpline, interp1d\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "# set number of points in resample of all spectra\n",
        "pt_res= 800\n",
        "\n",
        "def spliner(v,u):\n",
        "  \n",
        "  R=.5 # 0.5 ang. is a good example of our low-cut for R-space background to avoid in EXAFS \n",
        "  num_kts=int(1+(2.*R/(3.14159265359))*(0.512393)*np.sqrt(u[-1]))  #define number of knots based similar approach as xray-larch \n",
        "  #num_kts=9  ### possible alternative to hard-code number of spline knots\n",
        "  try:\n",
        "    # Could be used to define peaks that would bound the background spline fit  \n",
        "    #peaks, _=find_peaks(v,prominence=.033)\n",
        "    #pk_pts=[v[i] for i in peaks]\n",
        "    #xpk_pts=[u[i] for i in peaks]\n",
        "\n",
        "    # Define knot array for bkg\n",
        "    maxe=u[-1]\n",
        "    kts=[(maxe/num_kts)*i+1 for i in range(1,num_kts-1)]\n",
        "    u.insert(0,0.0)\n",
        "    v.insert(0,0.0)\n",
        "\n",
        "    # Slow oscillating background with <15 splines\n",
        "    bkg=LSQUnivariateSpline(x=u, y=v,t=kts)\n",
        "    pt_res= 800\n",
        "\n",
        "    # Interpolation function of mu,e for resampling\n",
        "    inter=interp1d(x=u,y=v)\n",
        "    grid=[(maxe/pt_res)*nn for nn in range(pt_res)]\n",
        "\n",
        "    # Uncomment and return subsline to see the bkg-spline that is taken off\n",
        "    #subspline=[float(j-bkg(i)) for i,j in zip(u,v)]\n",
        "\n",
        "    # Result of subtracting slow-moving bkg from resampled mu\n",
        "    bkg_grid=[float(inter(i)-bkg(i)) for i in grid]\n",
        "\n",
        "    output=bkg_grid\n",
        "\n",
        "  except:\n",
        "    output=[22.,33.]\n",
        "    # a dummy output for when knot-definer fails\n",
        "\n",
        "   \n",
        "  return output\n",
        "\n",
        "# init and apply above defined UDF\n",
        "uspliner=udf(spliner,'array<float>')\n",
        "ndf=zdf2.withColumn(\"norm_e\", uspliner(zdf2[\"mu_zset\"], zdf2[\"e_zset\"]))\n",
        "\n",
        "#Filter out the failures\n",
        "from pyspark.sql.functions import size\n",
        "ndf1=ndf.filter(size('norm_e') > 4)\n",
        "\n",
        "# Convert to pandas dataframe and save, zip, download\n",
        "ndf_out=ndf.toPandas()\n",
        "ndf_out.to_csv(\"data_agg_mini_cleanest.csv\")\n",
        "!zip data_agg_mini_cleanest.zip data_agg_mini_cleanest.csv\n",
        "import time\n",
        "\n",
        "# May fail to download, as sleeptime is used to wait for zip file to be recognized in colab drive\n",
        "time.sleep(10)\n",
        "from google.colab import files\n",
        "files.download(\"data_agg_mini_cleanest.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbCOVbGCwIZZ",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can use the process the normalized (and homogenously sampled) XAS spectra. In clustering regions of audio signals, [MFCC](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) arrays are useful. In following established techniques, I found this transformation very useful in conjunction with DBSCAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcnOOa2XDV-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType,FloatType\n",
        "import librosa\n",
        "from librosa import display\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#########\n",
        "# Constants tweaked to work\n",
        "pt_res=800\n",
        "nfft=180\n",
        "hopl=nfft+1\n",
        "sri=800\n",
        "##########\n",
        "\n",
        "def mffcer(u):\n",
        "  try:\n",
        "    # reshape and weight mu by e (like in k*chi(k)), then transform list -> array\n",
        "    bb=np.transpose(np.array(u))\n",
        "    bb=[float(i*bb[i]) for i in range(len(u))]\n",
        "    cc=np.array(bb)\n",
        "\n",
        "    # Librosa package contains wealth of audio signal processing functions\n",
        "    # We use it to stft -> mel spectrogram -> mfcc\n",
        "    spec=librosa.stft(y=cc,n_fft=nfft)\n",
        "    melspec=librosa.feature.melspectrogram(y=cc,S=spec)\n",
        "    mffc_ex=librosa.feature.mfcc(y=cc, sr=sri, S=melspec, n_mfcc=50000)\n",
        "\n",
        "    # Unpack the 2D array into 1D array, then assure they are floats\n",
        "    flatmf=np.absolute(mffc_ex).flatten()\n",
        "    flatmf2=[float(i) for i in flatmf]\n",
        "    output=flatmf2\n",
        "\n",
        "  except:\n",
        "    output=[[22.,33.]]\n",
        "\n",
        "   \n",
        "  return output\n",
        "\n",
        "# Init and apply MFCC UDF\n",
        "spectralize=udf(mffcer, ArrayType(FloatType()))\n",
        "mfcc_df=ndf1.withColumn(\"mfcc\", spectralize(ndf1[\"norm_e\"]))\n",
        "\n",
        "#mfcc_df.select(\"mfcc\").show()\n",
        "# Reduce dataframe to the only needed columns\n",
        "df=mfcc_df.select([c for c in mfcc_df.columns if c  in {'_2','_5','mfcc','4'}])\n",
        "\n",
        "## Saving, zipping and downloading the stuff\n",
        "df_out=df.toPandas()\n",
        "df_out.to_csv(\"mfcc_edge_2col_gmini.csv\")\n",
        "!zip mfcc_gmini.zip mfcc_edge_2col_gmini.csv\n",
        "time.sleep(7)\n",
        "from google.colab import files\n",
        "!ls -lht\n",
        "files.download(\"mfcc_gmini.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}